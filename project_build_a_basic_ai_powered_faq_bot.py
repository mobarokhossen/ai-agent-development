# -*- coding: utf-8 -*-
"""Project: Build a Basic AI-Powered FAQ Bot_Mobarok_Hossen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jYw3-i0vNZlpydBGbeHYnwnk02M4qR4K
"""

# Install Dependencies
!pip install -q langchain faiss-cpu pypdf langchain-community
!pip install -q sentence-transformers
!pip install -q ollama
!pip install -q langchain-ollama
!pip install -q langchain-core

# Import Libraries
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import Ollama
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import WebBaseLoader

import bs4
# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://ai.pydantic.dev/#next-steps",),

)
docs = loader.load()

print(f"Loaded {len(docs)} documents")

print(docs)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=50
)
chunks = splitter.split_documents(docs)
print(f"Split into {len(chunks)} chunks")

for index, i in enumerate(chunks[:2]):
  print(f"Chunk Number ",index+1)
  print(i)
  print("#####################")

!sudo apt update
!sudo apt install -y pciutils
!curl -fsSL https://ollama.com/install.sh | sh

import threading
import subprocess
import time

def run_ollama_serve():
  subprocess.Popen(["ollama", "serve"])

thread = threading.Thread(target=run_ollama_serve)
thread.start()
time.sleep(5)

!ollama pull all-minilm

!ollama pull gemma3

from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings

llm=ChatOllama(
    model="gemma3",
    temperature=0.5
)

embedding_model=OllamaEmbeddings(model="all-minilm")

vectorstore = FAISS.from_documents(chunks, embedding_model)
print("‚úÖ Vectorstore created with FAISS")

vectorstore.similarity_search("pydantic Ai",k=5)

vectorstore.similarity_search("llms.txt",k=5)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

questions = [
    "What is PydanticAI and why was it created?",
    "How is PydanticAI similar to FastAPI in terms of developer experience?",
    "What makes PydanticAI type-safe and structured?",
    "How does PydanticAI support streaming and debugging?",
    "What is llms.txt?"
]

for question in questions:
    print(f"\n‚ùì Question: {question}")
    result = qa_chain.invoke({"query": question})
    print("üìé Answer:", result['result'])